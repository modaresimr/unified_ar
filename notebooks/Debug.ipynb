{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and install Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 08:18:38.382884: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-21 08:18:38.385486: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-21 08:18:38.443002: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-21 08:18:38.443922: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-21 08:18:40.367876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n",
      "Please install GPU version of TF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 08:18:47.655656: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-11-21 08:18:47.679251: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import unified_ar as ar\n",
    "ar.reload()\n",
    "import os\n",
    "while \"datasets\" not in os.listdir():\n",
    "    os.chdir(\"..\")\n",
    "# import unified_ar.general.libinstall\n",
    "from unified_ar.constants import methods\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ipywidgets import interact,interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659ef0a1a1b24a1c817e321a9c8e1ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='dataset', options={'Home1': 0, 'Home2': 1, 'Aruba': 2, 'KaryoAdlNo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(dataset={d['method']().data_dscr:i for i,d in enumerate(methods.dataset)})\n",
    "def dataset_select(dataset=0):\n",
    "    global datasetdscr\n",
    "    datasetdscr=methods.dataset[dataset]['method']().load()\n",
    "    # datasetdscr = methods.dataset[2m]['method']().load()\n",
    "    print(f'dataset {datasetdscr.data_dscr} loaded')\n",
    "    datasetdscr.activities_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetdscr.activity_events['Activity'].value_counts().to_dict()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Visualize DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unified_ar as ar\n",
    "ar.reload()\n",
    "from unified_ar.datatool import dataset_viewer as dv\n",
    "\n",
    "dv.displaycontent(datasetdscr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.reload()\n",
    "\n",
    "# for act in datasetdscr.activities_map:\n",
    "# \tresult_analyse.dataset_viewer.view(datasetdscr,act)\n",
    "dv.sensor_hitmap(datasetdscr)\n",
    "print('sensor hitmap on begin, middle and end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.reload()\n",
    "# for act in datasetdscr.activities_map:\n",
    "dv.plotAct(datasetdscr,datasetdscr.activity_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 08:18:56.579717: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "ar.reload()\n",
    "evalmethod=ar.evaluation.SplitEval.SplitEval()\n",
    "evalmethod.dataset=datasetdscr\n",
    "Train,Test=evalmethod.makeTrainTest(datasetdscr.sensor_events,datasetdscr.activity_events)\n",
    "Train.acts=list(datasetdscr.activities_map.keys())\n",
    "Test.acts=list(datasetdscr.activities_map.keys())\n",
    "Train.act_map=datasetdscr.activities_map\n",
    "Test.act_map=datasetdscr.activities_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 08:18:58.679448: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "ar.reload()\n",
    "preprocessing=methods.preprocessing[0]['method']()\n",
    "TrainData=preprocessing.process(datasetdscr,Train)\n",
    "TestData=preprocessing.process(datasetdscr,Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 08:19:00.578907: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e914a3060c4b38950808be7ea35188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='segmentation', options={\"FixedEventWindow [{'size': 15, 'range': […"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ar.reload()\n",
    "methods=ar.constants.methods\n",
    "@interact_manual(segmentation={v['method']().shortname() + ' ' + (str(v['params']) if 'params' in v and len(v['params']) > 0 else ''): k for k, v in enumerate(methods.segmentation)})\n",
    "def segmentation_process(segmentation=0):\n",
    "    global Strain,Stest\n",
    "    # utils.reload()\n",
    "    prepare_segment2=ar.segmentation.segmentation_abstract.prepare_segment2\n",
    "    func=ar.Data('func');func.ui_debug={'seg':1};segindx=segmentation\n",
    "    func.acts=TrainData.acts\n",
    "    func.segmentor=methods.segmentation[segindx]['method']()\n",
    "    func.segmentor.applyDefParams(methods.segmentation[segindx]['params'])\n",
    "    func.activityFetcher=methods.activity_fetcher[0]['method']()  \n",
    "    Strain=prepare_segment2(func,TrainData,datasetdscr,train=True)\n",
    "    Stest=prepare_segment2(func,TestData,datasetdscr,train=False)\n",
    "    print('Segmentation Finished %d Train segment %d Test segment created %s' % (len(Strain.set_window),len(Stest.set_window), func.segmentor.shortname()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Dataset: only available for SWMeta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'save_data/meta_dataset/temp_0/data.pkl.lz4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m meta_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadState\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmeta_dataset/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmethods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_names\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# import unified_ar.general.utils as utils\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# import pandas as pd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# meta_dataset=utils.loadState(f'meta_dataset/220816_08-30-22-Home1-s=1 0')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# meta_dataset['meta_targets']=pd.DataFrame(meta_dataset['meta_targets'])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# meta_dataset['meta_features']=pd.DataFrame(meta_dataset['meta_features'])\u001b[39;00m\n\u001b[1;32m      7\u001b[0m display(meta_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta_features\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/unified_ar/unified_ar/general/utils.py:155\u001b[0m, in \u001b[0;36mloadState\u001b[0;34m(file, name, raiseException)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# if(name=='data'):\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# from unified_ar.metric.CMbasedMetric import CMbasedMetric\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# from unified_ar.metric.event_confusion_matrix import event_confusion_matrix\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m#         evalres[i]['test'].quality      =CMbasedMetric(data.event_cm,'macro',None)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m#     return [run_info,datasetdscr,evalres]\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mlz4frame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpklfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.lz4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    156\u001b[0m     data \u001b[38;5;241m=\u001b[39m ModuleRenamer(file)\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/lz4/frame/__init__.py:878\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(filename, mode, encoding, errors, newline, block_size, block_linked, compression_level, content_checksum, block_checksum, auto_flush, return_bytearray, source_size)\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewline\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not supported in binary mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    876\u001b[0m _mode \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 878\u001b[0m binary_file \u001b[38;5;241m=\u001b[39m \u001b[43mLZ4FrameFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_linked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_linked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent_checksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_checksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock_checksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_flush\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_flush\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_bytearray\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_bytearray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(binary_file, encoding, errors, newline)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/lz4/frame/__init__.py:530\u001b[0m, in \u001b[0;36mLZ4FrameFile.__init__\u001b[0;34m(self, filename, mode, block_size, block_linked, compression_level, content_checksum, block_checksum, auto_flush, return_bytearray, source_size)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m    529\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 530\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closefp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode_code\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'save_data/meta_dataset/temp_0/data.pkl.lz4'"
     ]
    }
   ],
   "source": [
    "meta_dataset=ar.utils.loadState(f'meta_dataset/{methods.run_names[\"out\"]}_0')\n",
    "# import unified_ar.general.utils as utils\n",
    "# import pandas as pd\n",
    "# meta_dataset=utils.loadState(f'meta_dataset/220816_08-30-22-Home1-s=1 0')\n",
    "# meta_dataset['meta_targets']=pd.DataFrame(meta_dataset['meta_targets'])\n",
    "# meta_dataset['meta_features']=pd.DataFrame(meta_dataset['meta_features'])\n",
    "display(meta_dataset['meta_features'])\n",
    "display(meta_dataset['meta_targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0275e5fe6f8d4ffd9171720de390f837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='feature_ex', options={\"SensorWord [{'vocab_size': 1000, 'var': 'vo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ar.reload()\n",
    "@interact_manual(feature_ex={v['method']().shortname() + ' ' + (str(v['params']) if 'params' in v and len(v['params']) > 0 else ''): k for k, v in  enumerate(methods.feature_extraction)})\n",
    "def feature_extraction_process(feature_ex=0):\n",
    "    ar.reload()\n",
    "    featureExtraction=ar.feature_extraction.feature_abstract.featureExtraction\n",
    "    feat=methods.feature_extraction[feature_ex]\n",
    "    featureExtractor=feat['method']()\n",
    "    featureExtractor.applyDefParams(feat['params'])\n",
    "    Strain.set=featureExtraction(featureExtractor,datasetdscr,Strain,istrain=True)\n",
    "    Stest.set =featureExtraction(featureExtractor,datasetdscr,Stest,istrain=False)\n",
    "    print('FeatureExtraction Finished train shape %s , test shape %s , %s' % (str(Strain.set.shape),str(Stest.set.shape), featureExtractor.shortname()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Feature Importance \n",
    "(not usable for SensorWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFeatureImportance(dataset):\n",
    "\tfrom sklearn.ensemble import ExtraTreesClassifier\n",
    "\timport matplotlib.pyplot as plt\n",
    "\tX=pd.DataFrame(dataset.set)\n",
    "\ty=dataset.label\n",
    "\tmodel = ExtraTreesClassifier()\n",
    "\tmodel.fit(X,y)\n",
    "\t# print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "\t#plot graph of feature importances for better visualization\n",
    "\tfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "\tfeat_importances.nlargest(20).plot(kind='barh')\n",
    "\tplt.show()\n",
    "\n",
    "print('important feature in train set')\n",
    "plotFeatureImportance(Strain)\n",
    "print('without cheating it should be similar in test set')\n",
    "plotFeatureImportance(Stest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Features Correlation \n",
    "(not usable for SensorWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def dataCorr(dataset):\n",
    "\timport seaborn as sns\n",
    "\tdata=pd.DataFrame(dataset.set)\n",
    "\tcorrmat = data.corr()\n",
    "\ttop_corr_features = corrmat.index\n",
    "\tplt.figure(figsize=(20,20))\n",
    "\t#plot heat map\n",
    "\tg=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n",
    "\tplt.show()\n",
    "\n",
    "print('data correlation in train set')\n",
    "dataCorr(Strain)\n",
    "print('without cheating, it should be similar, data correlation in test set')\n",
    "dataCorr(Stest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 08:18:17.811869: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afb20bb13244d6fa23730666bcad271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='classifier_method', options={'FCN': 0}, value=0), Button(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ar.reload()\n",
    "@interact_manual(classifier_method={d['method']().shortname():i for i,d in enumerate(methods.classifier)})\n",
    "def segmentation_process(classifier_method=0):\n",
    "\tglobal classifier\t\n",
    "\tcmethod=classifier_method\n",
    "\tclassifier=methods.classifier[cmethod]['method']()\n",
    "\tclassifier.applyDefParams(methods.classifier[cmethod]['params'])\n",
    "\tclassifier.createmodel(Strain.set[0].shape,len(Strain.acts),update_model=0)\n",
    "\tclassifier.setWeight(None)\n",
    "\tprint('Classifier model created  %s' % (classifier.shortname()))\n",
    "\tclassifier.train(Strain.set, Strain.label) \n",
    "\tprint('Classifier model trained  %s' % (classifier.shortname()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predicted        =classifier.predict(Strain.set)\n",
    "train_predicted_classes=classifier.predict_classes(Strain.set)    \n",
    "\n",
    "test_predicted        =classifier.predict(Stest.set)\n",
    "test_predicted_classes=classifier.predict_classes(Stest.set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# display classic metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.reload()\n",
    "from matplotlib.pylab import plt\n",
    "import unified_ar.result_analyse.visualisation as vs\n",
    "from unified_ar.metric.CMbasedMetric import CMbasedMetric\n",
    "from sklearn.metrics  import confusion_matrix\n",
    "\n",
    "train_cm=confusion_matrix(Strain.label,train_predicted_classes)\n",
    "test_cm=confusion_matrix(Stest.label,test_predicted_classes)\n",
    "activities=datasetdscr.activities\n",
    "remove_none=False\n",
    "if remove_none:\n",
    "    train_cm=train_cm[1:,1:]\n",
    "    test_cm=test_cm[1:,1:]\n",
    "    activities=datasetdscr.activities[1:]\n",
    "\n",
    "fig,axs=plt.subplots(1,2,sharex=True,sharey=True)\n",
    "vs.plot_CM_normal(train_cm,datasetdscr.activities,title='train cm',ax=axs[0])\n",
    "print('train weighted avg',CMbasedMetric(train_cm,'weighted'));\n",
    "print('train micro avg',CMbasedMetric(train_cm,'micro'));\n",
    "print('train macro avg',CMbasedMetric(train_cm,'macro'));\n",
    "\n",
    "\n",
    "\n",
    "vs.plot_CM_normal(test_cm,datasetdscr.activities,title='test cm',ax=axs[1])\n",
    "print('test weighted avg',CMbasedMetric(test_cm,'weighted'));\n",
    "print('test micro avg',CMbasedMetric(test_cm,'micro'));\n",
    "print('test macro avg',CMbasedMetric(test_cm,'macro'));\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.reload()\n",
    "import unified_ar.combiner.SimpleCombiner\n",
    "combiner=methods.combiner[0]['method']()\n",
    "combiner.applyDefParams(methods.combiner[0]['params'])\n",
    "train_pred_events      =combiner.combine(Strain.s_event_list,Strain.set_window,train_predicted)\n",
    "print('train events merged  %s' % (combiner.shortname()))\n",
    "test_pred_events      =combiner.combine(Stest.s_event_list,Stest.set_window,test_predicted)\n",
    "print('test events merged  %s' % (combiner.shortname()))\n",
    "import unified_ar.result_analyse.visualisation as vs\n",
    "\n",
    "vs.plot_joint_events(datasetdscr, Train.a_events, train_pred_events)\n",
    "vs.plot_joint_events(datasetdscr, Test.a_events, test_pred_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS CM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.reload()\n",
    "from unified_ar.metric.CMbasedMetric import CMbasedMetric\n",
    "from unified_ar.metric.event_confusion_matrix import event_confusion_matrix\n",
    "\n",
    "train_event_cm     = event_confusion_matrix(Strain.a_events,train_pred_events,datasetdscr.activities)\n",
    "test_event_cm      = event_confusion_matrix(Stest.a_events,test_pred_events,datasetdscr.activities)\n",
    "\n",
    "activities=datasetdscr.activities\n",
    "if remove_none:\n",
    "    train_event_cm=train_event_cm[1:,1:]\n",
    "    test_event_cm=test_event_cm[1:,1:]\n",
    "    activities=datasetdscr.activities[1:]\n",
    "\n",
    "    \n",
    "fig,axs=plt.subplots(1,2,sharex=True,sharey=True)\n",
    "vs.plot_CM_normal(train_event_cm,datasetdscr.activities,title='train cm',ax=axs[0])\n",
    "print('train weighted avg',CMbasedMetric(train_event_cm,'weighted'));\n",
    "print('train micro avg',CMbasedMetric(train_event_cm,'micro'));\n",
    "print('train macro avg',CMbasedMetric(train_event_cm,'macro'));\n",
    "\n",
    "vs.plot_CM_normal(test_event_cm,activities,title='test cm',ax=axs[1])\n",
    "print('test weighted avg',CMbasedMetric(test_event_cm,'weighted'));\n",
    "print('test micro avg',CMbasedMetric(test_event_cm,'micro'));\n",
    "print('test macro avg',CMbasedMetric(test_event_cm,'macro'));\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar.reload()\n",
    "import unified_ar.result_analyse.visualisation as vs\n",
    "\n",
    "\n",
    "# vs.plot_per_act_cm(datasetdscr,train_cm,train_event_cm)\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.show()\n",
    "vs.plot_per_act_cm(datasetdscr,test_cm,test_event_cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
