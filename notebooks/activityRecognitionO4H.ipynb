{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bc3891c-5d37-43ea-ae62-fca179de2811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from numpy import array\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from numpy.lib.stride_tricks import as_strided as ast\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from matplotlib import pyplot\n",
    "from numpy.lib.stride_tricks import as_strided as ast\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras import regularizers\n",
    "        \n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import pickle\n",
    "\n",
    "import datetime\n",
    "\n",
    "# CuDNNLSTM is not available in Keras v2, you would need to use the standard LSTM layer\n",
    "# from keras.layers import CuDNNLSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d57e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6663dc31-1b6e-441f-8344-8862130b0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluateWeightedPerformance(cm):\n",
    "    TP = np.diag(cm)\n",
    "    FP = np.sum(cm, axis=0) - TP\n",
    "    FN = np.sum(cm, axis=1) - TP\n",
    "    summ = np.sum(cm, axis=1)\n",
    "    TN = []\n",
    "    precision =[]\n",
    "    FMeaure = []\n",
    "    recall = []\n",
    "    accuracy = []\n",
    "    for i in range(len(TP)):\n",
    "        temp = np.delete(cm, i, 0)    # delete ith row\n",
    "        temp = np.delete(temp, i, 1)  # delete ith column\n",
    "        TN.append(sum(sum(temp)))\n",
    "    message = \"TP = \" + str(TP)+\"\\n\"\n",
    "    print(message)\n",
    "    message = \"FP = \" + str(FP)+\"\\n\"\n",
    "    print(message)\n",
    "    message = \"FN = \" + str(FN)+\"\\n\"\n",
    "    print(message)\n",
    "    message = \"TN = \" + str(TN)+\"\\n\"\n",
    "    print(message)\n",
    "    for i in range(0, len(TP)):\n",
    "        if(TP[i] == 0 and FP[i] == 0):\n",
    "            precision.append(0)\n",
    "            FMeaure.append(0)\n",
    "            recall.append(TP[i]/(TP[i]+FN[i]))\n",
    "            accuracy.append((TN[i]+TP[i])/(TN[i]+FP[i]+FN[i]+TP[i]))\n",
    "        else:\n",
    "            precision.append(TP[i]/(TP[i]+FP[i]))\n",
    "            recall.append(TP[i]/(TP[i]+FN[i]))\n",
    "            FMeaure.append(2*(precision[i]*recall[i])/(recall[i]+precision[i]))\n",
    "            accuracy.append((TN[i]+TP[i])/(TN[i]+FP[i]+FN[i]+TP[i]))\n",
    "##    message = \"precision \"+str(precision)+\"\\n\"\n",
    "##    print(message)\n",
    "##    message = \"recall \"+str(recall)+\"\\n\"\n",
    "##    print(message)\n",
    "##    message = \"FMeaure \"+str(FMeaure)+\"\\n\"\n",
    "##    print(message)\n",
    "##    message = \"accuracy \"+str(accuracy)+\"\\n\"\n",
    "##    print(message)\n",
    "    sumsum = 0\n",
    "    weightedaccuracy = 0\n",
    "    weightedFMeaure= 0\n",
    "    weightedrecall = 0\n",
    "    weightedprecision = 0\n",
    "    for i in range(0, len(precision)):\n",
    "        weightedprecision = weightedprecision+precision[i]*summ[i]\n",
    "        weightedrecall = weightedrecall+recall[i]*summ[i]\n",
    "        weightedFMeaure = weightedFMeaure+FMeaure[i]*summ[i]\n",
    "        weightedaccuracy = weightedaccuracy+accuracy[i]*summ[i]\n",
    "        sumsum = summ[i]+sumsum\n",
    "    weightedprecision = weightedprecision/sumsum\n",
    "    weightedrecall = weightedrecall/sumsum\n",
    "    weightedFMeaure = weightedFMeaure/sumsum\n",
    "    weightedaccuracy = weightedaccuracy/sumsum\n",
    "    precision = np.average(precision)\n",
    "    recall = np.average(recall)\n",
    "    FMeaure = np.average(FMeaure)\n",
    "    accuracy = np.average(accuracy)\n",
    "##    message = \"Average precision \"+str(precision)+\"\\n\"\n",
    "##    print(message)\n",
    "##    message = \"Average recall \"+str(recall)+\"\\n\"\n",
    "##    print(message)\n",
    "##    message = \"Average FMeaure \"+str(FMeaure)+\"\\n\"\n",
    "##    print(message)\n",
    "##    message = \"Average accuracy \"+str(accuracy)+\"\\n\"\n",
    "##    print(message)\n",
    "    message = \"Weighted precision \"+str(weightedprecision)+\"\\n\"\n",
    "    print(message)\n",
    "    message = \"Weighted recall \"+str(weightedrecall)+\"\\n\"\n",
    "    print(message)\n",
    "    message = \"Weighted FMeaure \"+str(weightedFMeaure)+\"\\n\"\n",
    "    print(message)\n",
    "    message = \"Weighted accuracy \"+str(weightedaccuracy)+\"\\n\"\n",
    "    print(message)\n",
    "    \n",
    "    \n",
    "def norm_shape(shape):\n",
    "    '''\n",
    "    Normalize numpy array shapes so they're always expressed as a tuple,\n",
    "    even for one-dimensional shapes.\n",
    "    Parameters\n",
    "        shape - an int, or a tuple of ints\n",
    "    Returns\n",
    "        a shape tuple\n",
    "    '''\n",
    "    try:\n",
    "        i = int(shape)\n",
    "        return (i,)\n",
    "    except TypeError:\n",
    "        # shape was not a number\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        t = tuple(shape)\n",
    "        return t\n",
    "    except TypeError:\n",
    "        # shape was not iterable\n",
    "        pass\n",
    "\n",
    "    raise TypeError('shape must be an int, or a tuple of ints')\n",
    "\n",
    "def sliding_window(a,ws,ss = None,flatten = True):\n",
    "    '''\n",
    "    Return a sliding window over a in any number of dimensions\n",
    "    Parameters:\n",
    "        a  - an n-dimensional numpy array\n",
    "        ws - an int (a is 1D) or tuple (a is 2D or greater) representing the size\n",
    "             of each dimension of the window\n",
    "        ss - an int (a is 1D) or tuple (a is 2D or greater) representing the\n",
    "             amount to slide the window in each dimension. If not specified, it\n",
    "             defaults to ws.\n",
    "        flatten - if True, all slices are flattened, otherwise, there is an\n",
    "                  extra dimension for each dimension of the input.\n",
    "    Returns\n",
    "        an array containing each n-dimensional window from a\n",
    "    '''\n",
    "\n",
    "    if None is ss:\n",
    "        # ss was not provided. the windows will not overlap in any direction.\n",
    "        ss = ws\n",
    "    ws = norm_shape(ws)\n",
    "    ss = norm_shape(ss)\n",
    "    print(str(ws))\n",
    "    print(str(ss))\n",
    "\n",
    "    # convert ws, ss, and a.shape to numpy arrays so that we can do math in every\n",
    "    # dimension at once.\n",
    "    ws = np.array(ws)\n",
    "    ss = np.array(ss)\n",
    "    shape = np.array(a.shape)\n",
    "    # ensure that ws, ss, and a.shape all have the same number of dimensions\n",
    "    ls = [len(shape),len(ws),len(ss)]\n",
    "    if 1 != len(set(ls)):\n",
    "        raise ValueError(\\\n",
    "        'a.shape, ws and ss must all have the same length. They were %s' % str(ls))\n",
    "\n",
    "    # ensure that ws is smaller than a in every dimension\n",
    "    if np.any(ws > shape):\n",
    "        raise ValueError(\\\n",
    "        'ws cannot be larger than a in any dimension.\\a.shape was %s and ws was %s' % (str(a.shape),str(ws)))\n",
    "\n",
    "    # how many slices will there be in each dimension?\n",
    "    newshape = norm_shape(((shape - ws) // ss) + 1)\n",
    "    # the shape of the strided array will be the number of slices in each dimension\n",
    "    # plus the shape of the window (tuple addition)\n",
    "    newshape += norm_shape(ws)\n",
    "    # the strides tuple will be the array's strides multiplied by step size, plus\n",
    "    # the array's strides (tuple addition)\n",
    "    newstrides = norm_shape(np.array(a.strides) * ss) + a.strides\n",
    "    strided = ast(a,shape = newshape,strides = newstrides)\n",
    "    if not flatten:\n",
    "        return strided\n",
    "\n",
    "    # Collapse strided so that it has one more dimension than the window.  I.e.,\n",
    "    # the new array is a flat list of slices.\n",
    "    meat = len(ws) if ws.shape else 0\n",
    "    firstdim = (np.product(newshape[:-meat]),) if ws.shape else ()\n",
    "    dim = firstdim + (newshape[-meat:])\n",
    "    return strided.reshape(dim)\n",
    "\n",
    "def opp_sliding_window(data_x, data_y, ws, ss, flatten=True):\n",
    "    data_x = sliding_window(data_x, (ws, data_x.shape[1]), (ss, 1), flatten)#(data_x, (ws, data_x.shape[1]), (ss, 1), flatten)\n",
    "    #data_x = sliding_window(data_x,ws, ss, flatten)\n",
    "    # This Command will assign the last sample class to be the window class\n",
    "    # data_y = np.asarray([[i[-1]] for i in sliding_window(data_y,ws,ss)])\n",
    "    # This Commands will assign the most dominant class to be the window class\n",
    "    windows = sliding_window(data_y, ws, ss, flatten)\n",
    "    mostDom = []\n",
    "    for i in windows:\n",
    "        mostDom.append(most_common(i))\n",
    "    data_y = np.asarray(mostDom)\n",
    "    return data_x.astype(np.float32), data_y.reshape(len(data_y)).astype(np.uint8)\n",
    "\n",
    "def most_common(L):\n",
    "    # get an iterable of (item, iterable) pairs\n",
    "    SL = sorted((x, i) for i, x in enumerate(L))\n",
    "    # print 'SL:', SL\n",
    "    groups = itertools.groupby(SL, key=operator.itemgetter(0))\n",
    "\n",
    "    # auxiliary function to get \"quality\" for an item\n",
    "    def _auxfun(g):\n",
    "        item, iterable = g\n",
    "        count = 0\n",
    "        min_index = len(L)\n",
    "        for _, where in iterable:\n",
    "            count += 1\n",
    "            min_index = min(min_index, where)\n",
    "        # print 'item %r, count %r, minind %r' % (item, count, min_index)\n",
    "        return count, -min_index\n",
    "\n",
    "    # pick the highest-count/earliest item\n",
    "    return max(groups, key=_auxfun)[0]\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(modelName):\n",
    "        # load dataset\n",
    "        #dataset = open(\"OnlyActivities.csv\", \"r\")\n",
    "        dataset = open(\"oa.csv\", \"r\")\n",
    "        \n",
    "        lines = dataset.readlines()\n",
    "        dataLength= len(lines)\n",
    "        number = 0\n",
    "        splitRateTrainTest = 0.7\n",
    "        # splitRateTrainTest = 1.0\n",
    "        trainSize = round(dataLength*splitRateTrainTest)\n",
    "        testSize = dataLength - trainSize\n",
    "        for x in lines:\n",
    "            if(number == 0):\n",
    "                partsV0 = x.split('\\n', 1)\n",
    "                featuresV0 = partsV0[0].split(',')\n",
    "                NumberofFeatures= len(featuresV0)\n",
    "                number = number +1\n",
    "        data = [[0 for k in range (NumberofFeatures)] for l in range (dataLength)]\n",
    "        re = 0\n",
    "        for x in lines:\n",
    "            parts = x.split('\\n', 1)\n",
    "            features = parts[0].split(',')\n",
    "            data[re] = features\n",
    "            re = re+1\n",
    "        data = array(data)\n",
    "        datafeature = data[:, 2:-1]\n",
    "        datalabel = data[:, -1]\n",
    "        WindowSize= 128\n",
    "        Overlapp= 64\n",
    "        if(modelName == \"CNN-LSTM\" or modelName == \"CNN\" or modelName == \"LSTM\"):\n",
    "            data, label = opp_sliding_window(datafeature, datalabel, WindowSize, Overlapp)\n",
    "        else:\n",
    "            data = datafeature\n",
    "            label = datalabel\n",
    "        datasizeAfterWindowing= len(data)\n",
    "        \n",
    "        testSize = int(0.3*datasizeAfterWindowing)\n",
    "        trainSize=datasizeAfterWindowing-testSize\n",
    "        print(f'trainsize:{trainSize} testsize:{testSize}')\n",
    "        trainX = data[0:trainSize, :]\n",
    "        trainy = label[0:trainSize]\n",
    "        testX = data[trainSize:, :]#[trainSize+1:(datasizeAfterWindowing),:]\n",
    "        testy = label[trainSize:]#label[trainSize+1:(datasizeAfterWindowing)]\n",
    "        trainy = to_categorical(trainy, num_classes=17)\n",
    "        testy = to_categorical(testy, num_classes=17)\n",
    "        return trainX, trainy, testX, testy\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "\tprint(scores)\n",
    "\tm, s = mean(scores), std(scores)\n",
    "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "20ee9bb3-bf88-4e2d-b6d1-e6384d377d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 235)\n",
      "(64, 1)\n",
      "(128,)\n",
      "(64,)\n",
      "trainsize:3626 testsize:1553\n"
     ]
    }
   ],
   "source": [
    "modelName = \"CNN-LSTM\"\n",
    "\n",
    "trainX, trainy, testX, testy = load_dataset(modelName)\n",
    "\n",
    "scores = list()\n",
    "precisions = list()\n",
    "recalls = list()\n",
    "f1s = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cdf736",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape,testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "118e92a2-02a6-4d7d-b94f-2189b2819b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_timesteps = \n",
      "128\n",
      "n_features = \n",
      "235\n",
      "n_outputs = \n",
      "17\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_18 (TimeD  (None, None, 126, 64)     45184     \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_19 (TimeD  (None, None, 124, 32)     6176      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_20 (TimeD  (None, None, 122, 16)     1552      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_21 (TimeD  (None, None, 122, 16)     0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_22 (TimeD  (None, None, 61, 16)      0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_23 (TimeD  (None, None, 976)         0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, None, 200)         941600    \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, None, 100)         120400    \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 100)               80400     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 80)                8080      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 17)                1377      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1204769 (4.60 MB)\n",
      "Trainable params: 1204769 (4.60 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "2023-09-07 17:10:56.917901\n",
      "Epoch 1/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.5397 - accuracy: 0.6795\n",
      "Epoch 1: val_accuracy improved from -inf to 0.72039, saving model to .\\weightsCNNLSTMAct12012022.best.hdf5\n",
      "290/290 [==============================] - 14s 27ms/step - loss: 1.5334 - accuracy: 0.6810 - val_loss: 1.2049 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "  5/290 [..............................] - ETA: 8s - loss: 1.5936 - accuracy: 0.6200 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/290 [==============================] - ETA: 0s - loss: 1.3176 - accuracy: 0.6848\n",
      "Epoch 2: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3176 - accuracy: 0.6848 - val_loss: 1.2089 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.3180 - accuracy: 0.6851\n",
      "Epoch 3: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3184 - accuracy: 0.6848 - val_loss: 1.2062 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.3189 - accuracy: 0.6844\n",
      "Epoch 4: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3172 - accuracy: 0.6848 - val_loss: 1.2061 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.3171 - accuracy: 0.6851\n",
      "Epoch 5: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3173 - accuracy: 0.6848 - val_loss: 1.2077 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "289/290 [============================>.] - ETA: 0s - loss: 1.3112 - accuracy: 0.6844\n",
      "Epoch 6: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3100 - accuracy: 0.6848 - val_loss: 1.2181 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "289/290 [============================>.] - ETA: 0s - loss: 1.3135 - accuracy: 0.6848\n",
      "Epoch 7: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 1.3130 - accuracy: 0.6848 - val_loss: 1.2028 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.3179 - accuracy: 0.6844\n",
      "Epoch 8: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3160 - accuracy: 0.6848 - val_loss: 1.2093 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.3124 - accuracy: 0.6844\n",
      "Epoch 9: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3114 - accuracy: 0.6848 - val_loss: 1.2074 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "289/290 [============================>.] - ETA: 0s - loss: 1.3108 - accuracy: 0.6848\n",
      "Epoch 10: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 6s 22ms/step - loss: 1.3103 - accuracy: 0.6848 - val_loss: 1.2076 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.3132 - accuracy: 0.6848\n",
      "Epoch 11: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 6s 22ms/step - loss: 1.3132 - accuracy: 0.6848 - val_loss: 1.2035 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "289/290 [============================>.] - ETA: 0s - loss: 1.3130 - accuracy: 0.6848\n",
      "Epoch 12: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3130 - accuracy: 0.6848 - val_loss: 1.2051 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.3070 - accuracy: 0.6848\n",
      "Epoch 13: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 1.3070 - accuracy: 0.6848 - val_loss: 1.2200 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.3069 - accuracy: 0.6848\n",
      "Epoch 14: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3069 - accuracy: 0.6848 - val_loss: 1.2086 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.3073 - accuracy: 0.6848\n",
      "Epoch 15: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 6s 22ms/step - loss: 1.3073 - accuracy: 0.6848 - val_loss: 1.2086 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.3002 - accuracy: 0.6868\n",
      "Epoch 16: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3065 - accuracy: 0.6848 - val_loss: 1.2218 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.3069 - accuracy: 0.6844\n",
      "Epoch 17: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3054 - accuracy: 0.6848 - val_loss: 1.2128 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "289/290 [============================>.] - ETA: 0s - loss: 1.3054 - accuracy: 0.6855\n",
      "Epoch 18: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3073 - accuracy: 0.6848 - val_loss: 1.2182 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.3057 - accuracy: 0.6844\n",
      "Epoch 19: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 6s 22ms/step - loss: 1.3051 - accuracy: 0.6848 - val_loss: 1.2040 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.3067 - accuracy: 0.6848\n",
      "Epoch 20: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 1.3067 - accuracy: 0.6848 - val_loss: 1.2094 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "289/290 [============================>.] - ETA: 0s - loss: 1.3047 - accuracy: 0.6851\n",
      "Epoch 21: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3056 - accuracy: 0.6848 - val_loss: 1.2051 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.3059 - accuracy: 0.6844\n",
      "Epoch 22: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 25ms/step - loss: 1.3046 - accuracy: 0.6848 - val_loss: 1.2083 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.3055 - accuracy: 0.6848\n",
      "Epoch 23: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3055 - accuracy: 0.6848 - val_loss: 1.2205 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.3037 - accuracy: 0.6848\n",
      "Epoch 24: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 1.3037 - accuracy: 0.6848 - val_loss: 1.2049 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.3050 - accuracy: 0.6848\n",
      "Epoch 25: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 1.3050 - accuracy: 0.6848 - val_loss: 1.2116 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.3048 - accuracy: 0.6848\n",
      "Epoch 26: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 25ms/step - loss: 1.3048 - accuracy: 0.6848 - val_loss: 1.2052 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.3036 - accuracy: 0.6848\n",
      "Epoch 27: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 25ms/step - loss: 1.3036 - accuracy: 0.6848 - val_loss: 1.2108 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "289/290 [============================>.] - ETA: 0s - loss: 1.3051 - accuracy: 0.6848\n",
      "Epoch 28: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 1.3051 - accuracy: 0.6848 - val_loss: 1.2055 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "289/290 [============================>.] - ETA: 0s - loss: 1.3054 - accuracy: 0.6844\n",
      "Epoch 29: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 1.3041 - accuracy: 0.6848 - val_loss: 1.2109 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.3049 - accuracy: 0.6847\n",
      "Epoch 30: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3042 - accuracy: 0.6848 - val_loss: 1.2044 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "289/290 [============================>.] - ETA: 0s - loss: 1.3027 - accuracy: 0.6851\n",
      "Epoch 31: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3038 - accuracy: 0.6848 - val_loss: 1.2077 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.3041 - accuracy: 0.6848\n",
      "Epoch 32: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3041 - accuracy: 0.6848 - val_loss: 1.2085 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.3018 - accuracy: 0.6840\n",
      "Epoch 33: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3000 - accuracy: 0.6848 - val_loss: 1.2185 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.3024 - accuracy: 0.6848\n",
      "Epoch 34: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3024 - accuracy: 0.6848 - val_loss: 1.2121 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 35/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.3011 - accuracy: 0.6847\n",
      "Epoch 35: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3011 - accuracy: 0.6848 - val_loss: 1.2137 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "289/290 [============================>.] - ETA: 0s - loss: 1.3020 - accuracy: 0.6844\n",
      "Epoch 36: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 6s 22ms/step - loss: 1.3002 - accuracy: 0.6848 - val_loss: 1.2075 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.2985 - accuracy: 0.6861\n",
      "Epoch 37: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3015 - accuracy: 0.6848 - val_loss: 1.2063 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.2997 - accuracy: 0.6848\n",
      "Epoch 38: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.2997 - accuracy: 0.6848 - val_loss: 1.2072 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "289/290 [============================>.] - ETA: 0s - loss: 1.2960 - accuracy: 0.6855\n",
      "Epoch 39: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.2985 - accuracy: 0.6848 - val_loss: 1.2207 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "289/290 [============================>.] - ETA: 0s - loss: 1.3018 - accuracy: 0.6851\n",
      "Epoch 40: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 1.3022 - accuracy: 0.6848 - val_loss: 1.2091 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "289/290 [============================>.] - ETA: 0s - loss: 1.2996 - accuracy: 0.6844\n",
      "Epoch 41: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 24ms/step - loss: 1.2986 - accuracy: 0.6848 - val_loss: 1.2083 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.3004 - accuracy: 0.6851\n",
      "Epoch 42: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.3008 - accuracy: 0.6848 - val_loss: 1.2193 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.2992 - accuracy: 0.6848\n",
      "Epoch 43: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.2992 - accuracy: 0.6848 - val_loss: 1.2078 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.2971 - accuracy: 0.6848\n",
      "Epoch 44: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 25ms/step - loss: 1.2971 - accuracy: 0.6848 - val_loss: 1.2098 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.2983 - accuracy: 0.6844\n",
      "Epoch 45: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 8s 26ms/step - loss: 1.2982 - accuracy: 0.6848 - val_loss: 1.2084 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.2973 - accuracy: 0.6848\n",
      "Epoch 46: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.2973 - accuracy: 0.6848 - val_loss: 1.2335 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "288/290 [============================>.] - ETA: 0s - loss: 1.2992 - accuracy: 0.6840\n",
      "Epoch 47: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 6s 22ms/step - loss: 1.2969 - accuracy: 0.6848 - val_loss: 1.2130 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "289/290 [============================>.] - ETA: 0s - loss: 1.2968 - accuracy: 0.6848\n",
      "Epoch 48: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 8s 27ms/step - loss: 1.2965 - accuracy: 0.6848 - val_loss: 1.2114 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "289/290 [============================>.] - ETA: 0s - loss: 1.2929 - accuracy: 0.6851\n",
      "Epoch 49: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 6s 22ms/step - loss: 1.2934 - accuracy: 0.6848 - val_loss: 1.2113 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "290/290 [==============================] - ETA: 0s - loss: 1.2946 - accuracy: 0.6848\n",
      "Epoch 50: val_accuracy did not improve from 0.72039\n",
      "290/290 [==============================] - 7s 23ms/step - loss: 1.2946 - accuracy: 0.6848 - val_loss: 1.2119 - val_accuracy: 0.7204 - lr: 0.0010\n",
      "2023-09-07 17:10:56.917901\n",
      "2023-09-07 17:16:42.079382\n",
      "Saving History Model\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_18 (TimeD  (None, None, 126, 64)     45184     \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_19 (TimeD  (None, None, 124, 32)     6176      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_20 (TimeD  (None, None, 122, 16)     1552      \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_21 (TimeD  (None, None, 122, 16)     0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_22 (TimeD  (None, None, 61, 16)      0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " time_distributed_23 (TimeD  (None, None, 976)         0         \n",
      " istributed)                                                     \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, None, 200)         941600    \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, None, 100)         120400    \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 100)               80400     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 80)                8080      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 17)                1377      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1204769 (4.60 MB)\n",
      "Trainable params: 1204769 (4.60 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "verbose = 1 #0, 1000, 60\n",
    "\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "print(\"n_timesteps = \")\n",
    "print(n_timesteps)\n",
    "print(\"n_features = \")\n",
    "print(n_features)\n",
    "print(\"n_outputs = \")\n",
    "print(n_outputs)\n",
    "# reshape data into time steps of sub-sequences\n",
    "#n_steps, n_length = 4, 32\n",
    "# n_steps, n_length = 4, 32\n",
    "n_steps, n_length = 1, 32*4\n",
    "trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
    "model.add(TimeDistributed(Conv1D(filters=32, kernel_size=3, activation='relu')))\n",
    "model.add(TimeDistributed(Conv1D(filters=16, kernel_size=3, activation='relu')))\n",
    "model.add(TimeDistributed(Dropout(0.4)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(units=200, return_sequences=True))\n",
    "model.add(LSTM(units=100, return_sequences=True))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dropout(0.4))\n",
    "#model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(80, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "filepath=\"./weightsCNNLSTMAct12012022.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'loss', factor=0.5,\n",
    "                  patience=50, min_lr=0.0001)\n",
    "\n",
    "r = datetime.datetime.now()\n",
    "print (datetime.datetime.now())\n",
    "\n",
    "# fit network\n",
    "history = model.fit(trainX, trainy, epochs=50, batch_size=10, verbose=verbose, validation_split=0.2, callbacks = [reduce_lr,checkpoint])\n",
    "\n",
    "print (r)\n",
    "print (datetime.datetime.now())\n",
    "\n",
    "print (\"Saving History Model\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "314c6c97-c37e-4e32-bfdd-a22310f2d937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History Model Saved\n"
     ]
    }
   ],
   "source": [
    "with open('./trainHistory12012022', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "\n",
    "print (\"History Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1234272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 4, 32, 235), dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "03974c00-38f4-4663-85c0-e9dc9d30b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49/49 [==============================] - 1s 13ms/step\n",
      "len(predictionindex)=  1553\n",
      "len(testyindex) =  1553\n",
      "TP = [   0    0   22    0    0   24    0 1079    0    8    0    0    0    0\n",
      "   23    0    0]\n",
      "\n",
      "FP = [  2   0  29   0   0  70   0 223   0  13   0   0   0   0  60   0   0]\n",
      "\n",
      "FN = [10  0 28 21  0 92  9 17  9 30 37 23  4  3 30 58 26]\n",
      "\n",
      "TN = [1541, 1553, 1474, 1532, 1553, 1367, 1544, 234, 1544, 1502, 1516, 1530, 1549, 1550, 1440, 1495, 1527]\n",
      "\n",
      "Weighted precision 0.6365944805414114\n",
      "\n",
      "Weighted recall nan\n",
      "\n",
      "Weighted FMeaure nan\n",
      "\n",
      "Weighted accuracy 0.8765358285005155\n",
      "\n",
      "Accuracy: 0.744366\n",
      "Precision: 0.636594\n",
      "Recall: 0.744366\n",
      "F1 score: 0.684376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_14456\\4129582141.py:35: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  FMeaure.append(2*(precision[i]*recall[i])/(recall[i]+precision[i]))\n",
      "C:\\Users\\Ali\\AppData\\Local\\Temp\\ipykernel_14456\\4129582141.py:30: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  recall.append(TP[i]/(TP[i]+FN[i]))\n",
      "c:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions = tunedModel.predict(testX)\n",
    "prediction = []\n",
    "testyindex = []\n",
    "outputconfusionMatrixfile = open(\"confusion.txt\",\"w\")\n",
    "predictionindex = []\n",
    "for i in range(0, len(predictions)):\n",
    "        prediction.append(0)\n",
    "        predictionindex.append(0)\n",
    "        testyindex.append(0)\n",
    "\n",
    "for i in range(0, len(predictions)):\n",
    "        prediction[i]= np.round(predictions[i], 2)\n",
    "        for j in range(0, len(predictions[0])):\n",
    "                if(predictions[i][j] == max(predictions[i])):\n",
    "                        predictionindex[i] = j\n",
    "                if(testy[i][j] == max(testy[i])):\n",
    "                        testyindex[i] = j\n",
    "print(\"len(predictionindex)= \", str(len(predictionindex)))\n",
    "print(\"len(testyindex) = \", str(len(testyindex)))\n",
    "filewriter = open(\"CNN-LSTM-Orange4Home-Output.txt\", \"w\")\n",
    "for i in range(0, len(testyindex)):\n",
    "    message = str(i)+ \" \"+ str(testyindex[i])+\" \"+ str(predictionindex[i])+\"\\n\"\n",
    "    filewriter.write(message)\n",
    "filewriter.close()\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "cm = confusion_matrix(testyindex, predictionindex, labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])\n",
    "evaluateWeightedPerformance(cm)\n",
    "with open(\"confusionOnlyActivities.txt\", 'w') as f:\n",
    "        f.write(np.array2string(cm, separator=', '))\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = accuracy_score(testyindex, predictionindex)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp)\n",
    "precision = precision_score(testyindex, predictionindex, average='weighted')\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn)\n",
    "recall = recall_score(testyindex, predictionindex, average='weighted')\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn)\n",
    "f1 = f1_score(testyindex, predictionindex, average='weighted')\n",
    "print('F1 score: %f' % f1)\n",
    "outputconfusionMatrixfile.close()\n",
    "    #return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efe0b37a-7ef1-4dc6-b7c7-dbdb9c9a18df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import keras\n",
    "\n",
    "def modelImport():\n",
    "    tunedModel = load_model(\"./weightsCNNLSTMAct12012022.best.hdf5\")\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    tunedModel.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return tunedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62931af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "tunedModel.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d6f862-23b2-4547-8b9d-94dbab634eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(tunedModel, dataPrepared):\n",
    "    predictClass = tunedModel.predict(dataPrepared)\n",
    "    predictProba = tunedModel.predict_proba(dataPrepared)\n",
    "    classes = np.argmax(predictClass, axis=1)\n",
    "    Proba = np.argmax(predictProba, axis=1)\n",
    "    print (classes)\n",
    "    print (\"predictedProbability\",predictProba)\n",
    "    print (\"Probability\",Proba)\n",
    "    return classes, Proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30a50e7f-3ed8-4597-86f9-9182ad6bbb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tunedModel = modelImport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8332ac0f-7db4-4b4c-a837-c5689fd78d06",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\D\\Projects\\unified_ar\\notebooks\\activityRecognitionO4H.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/D/Projects/unified_ar/notebooks/activityRecognitionO4H.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainX \u001b[39m=\u001b[39m trainX\u001b[39m.\u001b[39mreshape((trainX\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], n_steps, n_length, n_features))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/D/Projects/unified_ar/notebooks/activityRecognitionO4H.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m testX \u001b[39m=\u001b[39m testX\u001b[39m.\u001b[39mreshape((testX\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], n_steps, n_length, n_features))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/D/Projects/unified_ar/notebooks/activityRecognitionO4H.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m predictions \u001b[39m=\u001b[39m tunedModel\u001b[39m.\u001b[39;49mpredict(testX)\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Ali\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:2579\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2575\u001b[0m                 callbacks\u001b[39m.\u001b[39mon_predict_batch_end(\n\u001b[0;32m   2576\u001b[0m                     end_step, {\u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: batch_outputs}\n\u001b[0;32m   2577\u001b[0m                 )\n\u001b[0;32m   2578\u001b[0m     \u001b[39mif\u001b[39;00m batch_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 2579\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2580\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnexpected result of `predict_function` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2581\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(Empty batch_outputs). Please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2582\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2583\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2584\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minformation of where went wrong, or file a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2585\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39missue/bug to `tf.keras`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2586\u001b[0m         )\n\u001b[0;32m   2587\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_end()\n\u001b[0;32m   2588\u001b[0m all_outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure_up_to(\n\u001b[0;32m   2589\u001b[0m     batch_outputs, potentially_ragged_concat, outputs\n\u001b[0;32m   2590\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "n_steps, n_length = 4, 32\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "predictions = tunedModel.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c5773-0194-4f3e-90ce-a18e4f20416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(tunedModel, dataPrepared):\n",
    "    predictClass = tunedModel.predict(dataPrepared)\n",
    "    predictProba = tunedModel.predict_proba(dataPrepared)\n",
    "    classes = np.argmax(predictClass, axis=1)\n",
    "    Proba = np.argmax(predictProba, axis=1)\n",
    "    print (classes)\n",
    "    print (\"predictedProbability\",predictProba)\n",
    "    print (\"Probability\",Proba)\n",
    "    return classes, Proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960b16c1-bca6-458e-a4b5-c7fc286cba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelsPrediction(classes):\n",
    "    LABELS = [\n",
    "            \"Entering\",\n",
    "            \"Going_up\",\n",
    "            \"Showering\",\n",
    "            \"Using_the_sink\",\n",
    "            \"Going_down\",\n",
    "            \"Watching_TV\",\n",
    "            \"Using_the_toilet\",\n",
    "            \"Computing\",\n",
    "            \"Preparing\",\n",
    "            \"Cooking\",\n",
    "            \"Eating\",\n",
    "            \"Washing_the_dishes\",\n",
    "            \"Cleaning\",\n",
    "            \"Dressing\",\n",
    "            \"Reading\",\n",
    "            \"Napping\",\n",
    "            \"Leaving\"]\n",
    "\n",
    "    if classes == 0:\n",
    "        x = LABELS[0]\n",
    "        print (\"The Activity is: \", x)\n",
    "    elif classes == 1:\n",
    "        x = LABELS[1]\n",
    "        print (\"The Activity is: \", x)\n",
    "    elif classes == 2:\n",
    "        x = LABELS[2]\n",
    "        print (\"The Activity is: \", x)\n",
    "    elif classes == 3:\n",
    "        x = LABELS[3]\n",
    "        print (\"The Activity is: \", x)\n",
    "    elif classes == 4:\n",
    "        x = LABELS[4]\n",
    "        print (\"The Activity is: \", x)\n",
    "    elif classes == 5:\n",
    "        x = LABELS[5]\n",
    "        print (\"The Activity is: \", x)\n",
    "    elif classes == 6:\n",
    "        x = LABELS[6]\n",
    "        print (\"The Activity is: \", x)\n",
    "    elif classes == 7:\n",
    "        x = LABELS[7]\n",
    "        print (\"The Activity is: \", x)\n",
    "    elif classes == 8:\n",
    "        x = LABELS[8]\n",
    "        print (\"The Activity is: \", x)\n",
    "    elif classes == 9:\n",
    "        x = LABELS[9]\n",
    "        print (\"The Activity is: \", x)\n",
    "    elif classes == 10:\n",
    "        x = LABELS[10]\n",
    "        print (\"The Activity is: \", x)\n",
    "    elif classes == 11:\n",
    "        x = LABELS[11]\n",
    "        print (\"The Activity is: \", x)\n",
    "    elif classes == 12:\n",
    "        x = LABELS[12]\n",
    "        print (\"The Activity is: \", x)\n",
    "    elif classes == 13:\n",
    "        x = LABELS[13]\n",
    "        print (\"The Activity is: \", x)\n",
    "    elif classes == 14:\n",
    "        x = LABELS[14]\n",
    "        print (\"The Activity is: \", x)\n",
    "    elif classes == 15:\n",
    "        x = LABELS[16]\n",
    "        print (\"The Activity is: \", x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0eac5b-95bc-4f46-8231-f9b28cddb524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5179, 4, 32, 235)\n",
      "(5179, 17)\n",
      "(5179, 4, 32, 235)\n",
      "(5179, 17)\n"
     ]
    }
   ],
   "source": [
    "print (trainX.shape)\n",
    "print (trainy.shape)\n",
    "print (testX.shape)\n",
    "print (testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6752dc4-a9b2-4fc8-875c-694dd0af81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fff = 559 #48997 #49010 #53083 #54522 #55048 #55904 #57580 #57601 #57727 #59836 #60565 #62344 #21212\n",
    "ss = testX[fff, testX.shape[1], testX.shape[2], testX.shape[3]]\n",
    "\n",
    "print (testX[fff].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d91a3-6251-4f69-8799-52fa07c6c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssss, proba = prediction(tunedModel, ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94a2e6-cd89-449c-ad93-56ab01f393ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted  = labelsPrediction(ssss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2fe396-47a0-402e-877f-5646ca5de6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5179/5179 [==============================] - 1s 266us/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predict_y = tunedModel.predict(testX, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df089d3d-5556-4f84-b701-3ac8c90cb102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5179,)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "classes = np.argmax(predict_y, axis=1)\n",
    "testy = np.argmax(testy, axis=0)\n",
    "print (classes.shape)\n",
    "print (testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9dc7b-65dc-410a-ae5a-e1f0a72427c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array 0 cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_894363/786100272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLABELS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/home/labuseraber/environment/testEnvironment/.conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/home/labuseraber/environment/testEnvironment/.conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/home/labuseraber/environment/testEnvironment/.conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \"\"\"\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/home/labuseraber/environment/testEnvironment/.conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \"\"\"\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/raid/home/labuseraber/environment/testEnvironment/.conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise TypeError(\n\u001b[0;32m--> 270\u001b[0;31m                 \u001b[0;34m\"Singleton array %r cannot be considered a valid collection.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m             )\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Check that shape is returning an integer or default to len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array 0 cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(testy, predict_y))\n",
    "print(classification_report(testy, predict_y, target_names=LABELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67168980-2c73-4e24-87db-36560b4e4f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba51d1-0045-4cf5-a3a0-89966d341833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training history\n",
    "import pickle\n",
    "\n",
    "history = pickle.load(open('./trainHistory', \"rb\"))\n",
    "\n",
    "s = history\n",
    "\n",
    "print (type(history))\n",
    "print (len(history['accuracy']))\n",
    "print (history['accuracy'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f0436-496b-4f94-950f-884aafd8d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACC = 0.98269\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e0ad3b-620a-4c3e-a9d5-537c8aab1c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
